\documentclass{sig-alternate-05-2015}

\begin{document}

\title{Software Testing, Validation and Verification Class Project Mid-Term Report}

\numberofauthors{1}

\author{
\alignauthor
Juan Manuel Florez Fandino, Raul Quinonez Tirado \\ 
       \affaddr{University of Texas at Dallas}\\
       \affaddr{800 W. Campbell Road}\\
       \affaddr{Richardson, TX}\\
       \email{\{jflorez, Rxq100020\}@utdallas.edu}
}

\maketitle
\begin{abstract}
Faults or bugs are an inevitable aspect of computer programming. Bug reports are normally used to let developers know 
there is an error in the code, but they rarely provide all the information necessary to determine where the bug must be 
fixed at first glance. In this project, we aim at addressing this issue by implementing an Information Retrieval (IR) 
system that will leverage the information found in bug reports to identify the possible locations in the source code 
where the fault is located.
\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%

%\begin{CCSXML}
%<ccs2012>
%<concept>
%<concept_id>10010583.10010717.10010733.10010734</concept_id>
%<concept_desc>Hardware~Bug detection, localization and diagnosis</concept_desc>
%<concept_significance>500</concept_significance>
%</concept>
%<concept>
%<concept_id>10002951.10003317.10003347.10003349</concept_id>
%<concept_desc>Information systems~Document filtering</concept_desc>
%<concept_significance>300</concept_significance>
%</concept>
%<concept>
%<concept_id>10002951.10003317.10003359.10003362</concept_id>
%<concept_desc>Information systems~Retrieval effectiveness</concept_desc>
%<concept_significance>300</concept_significance>
%</concept>
%<concept>
%<concept_id>10011007.10011074.10011099</concept_id>
%<concept_desc>Software and its engineering~Software verification and validation</concept_desc>
%<concept_significance>300</concept_significance>
%</concept>
%</ccs2012>
%\end{CCSXML}
%
%\ccsdesc[500]{Hardware~Bug detection, localization and diagnosis}
%\ccsdesc[300]{Information systems~Document filtering}
%\ccsdesc[300]{Information systems~Retrieval effectiveness}
%\ccsdesc[300]{Software and its engineering~Software verification and validation}


%
% End generated code
%

%
%  Use this command to print the description
%
\printccsdesc

\keywords{Bug localization; information retrieval; corpus creation}

\section{Introduction}
Due to the inherent complexity of software products, the occurrence of bugs is inevitable. Many different methods
are used to prevent and repair faults in source code. However, on occasion these faults are not identified during
testing, instead being uncovered by the final users. Bug reports are used by both open source communities and
software development companies, and they allow users  to communicate software failures to the development team.
These reports are usually submitted by making use of a bug-tracking system.
Once a bug report is received the usual process starts by assessing the severity of the bug and then assigning a
developer or group of developers the task of locating and fixing the bug.

However, locating bugs is usually not a straightforward process. Software systems that have the need of using
bug-tracking systems are usually too complex for any one person to identify the possible location of a bug at a
glance. Manually searching through the source code files would be a prohibitively time-consuming in addition to
being error-prone. This is why many approaches for automated bug localization have been proposed. % REF?

Some of these approaches exploit the fact that source code is essentially text to allow search over code bases the
same way search over natural-language texts would be carried out. Information retrieval (IR) is the discipline
that studies search over corpora of unstructured data, and these approaches apply techniques born in this
field of study to bug localization.
One of the ways of locating software faults using IR is using a textual bug report and 
a collection of source code files \cite{le2014}. The system then will output a ranked list of files where
the bug is likely to be located. Developers will then look at the highest ranked locations for the bug first. 
Our goal for this project is to test a particular approach on a set of bug reports collected by our team.

\section{Problem}
Identifying faulty files in a software project using bug reports as input.

\section{Background}

\subsection{IR-Based Bug Localization}
Information Retrieval (IR) can be defined as the process of finding material of an unstructured nature in a large 
collection to satisfy a certain information need \cite{manning2008}. One known application of this discipline 
is internet search engines. These allow efficient search over millions of different web pages with the information need 
being presented in the form of a natural-language \textit{query} by a user. IR usually operates on a \textit{corpus}, 
i.e. a collection of documents which can be natural-language text such as books or news articles, but also 
multimedia like images or videos, or any combination of these. The typical output of such a system is a ranked 
subset of the corpus which hopefully contains the documents that are most relevant to the user's information 
need among the first few entries.

Even though source code is technically not unstructured text, IR-based approaches to bug localization have shown 
promising results over recent years \cite{zhou2012, poshyvanyk2007, saha2013, rao2011, dit2013}. The idea is 
that with some preprocessing, the source code of a certain version of a software product can be indexed and given 
the same treatment as a corpus of natural-language documents. This allows stakeholders to query the codebase of 
a project using natural language and extract all kinds of information from it. This has the notable advantage of 
being very easy to implement, but the main disadvantage of relying on the code containing adequately named 
identifiers and meaningful comments \cite{saha2014}.

Most IR-based bug localization techniques use bug reports as queries. Some of them use additional information to
improve ranking performance, such as stack traces or dynamic data. % Ref?

% Explain VSM and why.

\subsection{Linking fixes in version history to issues in bug-tracking systems}
IR approaches normally have to be empirically tested in order to assess their performance. This process is 
normally carried out using special corpora for which some queries have been manually audited and all 
(or a good part) of the relevant results for each one are known \cite{harman2011}. 
The text corpora is then used to implement the statistical analysis.This also applies to the 
field of automatic bug localization, in which we talk about a \textit{gold set}, i.e. a set of bug reports 
connected to the related program elements that were modified to address each one, or \textit{fix set}.
Even though some approaches to automate this process have been proposed \cite{dallmeier2007}, they lack
the precision to construct a reasonably reliable gold set.

The method that most of these approaches propose is \cite{sliwerski2005}:
\begin{enumerate}
	\item Begin with the change history of a software project and the bug-tracking system for the project.
	A bug tracker is not necessary but it help increase the confidence in extracted candidate bug fixes.
	
	\item Examine the commit messages for the software release being considered and extract anything resembling
	an issue identifier. The specific format of these depends on the specific bug-tracking system used by the
	project, and could be simply a number, a URL or a string like \emph{PROJECT-0000} as is the case with
	Apache projects.
	
	\item Increase the confidence of the candidate bug ids extracted by querying the version control system.
	If the issue is marked as fixed or assigned to a developer, the confidence level for the candidate bug
	fix increases. Similarly, if the issue contains attachments referencing the files being committed,
	confidence increases.
\end{enumerate}

 As stated before, the reports will be collected from known bug tracking system such as BugZilla and JIRA. These reports will only contain reports of bugs that have been fixed and its location is already known. This will help us to evaluate our performance.However, the main problem of this kind of approach is that they report a considerable number of false positives, since
potentially any number included in a commit message for other reasons could be interpreted as an issue id
if it exists in the bug-tracking system by chance. Additionally, not all fixed bugs get marked as such in these
systems, and some of them get fixed without explicitly being assigned to a developer.

\section{Implementation Plan}
We propose the reimplementation for bug-localization tool known as BugLocator \cite{zhou2012}. This tool uses IR
methods to locate bugs in a code base using bug reports as input. The tool also leverages document length and
previous recorded bug fixes to improve the results. The idea behind this is that long code files are more likely
to contain bugs than shorter files, and files that have been fixed before are similarly more likely to contain
defects than those that haven't been modified as much. The technique uses file-level granularity, i.e. given %Ref
a bug report, the result will be a ranked list of source files.

As it is true for most IR applications, it is necessary to index the source code in order to enable efficient 
search over it. For this, some preprocessing must be applied to the source files:

\begin{enumerate}
	\item First, the correct versions for the software project under study must be selected. The gold set will be
	collected for one or more versions of the source code. Since it only makes sense to look for a bug in the version
	in which it was reported, these different versions must be identified and used separately.
	
	\item Since the chosen granularity chosen for this application is file-level, the following step is iterating through
	the project source files and applying the next steps.
	
	\item Extract comments, literal strings and identifiers from each file. A parser for the particular programming
	language is not mandatory but will help identify these elements unambiguously. It is necessary to ignore reserved 
	words of the language and programming tokens such as \texttt{\{} since these don't contain any semantic information
	that would boost search performance.
	
	\item Apply identifier splitting. This process consists in recognizing composite identifiers such as 
	\texttt{threadExecutor} and splitting them into their constituent words, in this case \texttt{thread} and 
	\texttt{executor}. The original identifier is kept as well, since this has been shown to improve search
	effectiveness \cite{saha2013}.
	
	\item Eliminate stop words. These are words that don't contain meaningful information from the perspective of IR, 
	such as conjunctions, pronouns and articles (\textit{for}, \textit{about}, \textit{it}, etc.) This is because they
	appear in most documents, which from the perspective of information theory means they don't carry much
	information.
	
	\item Apply stemming. This is a process that reduces words to their base form, e.g. ``implementing'' and 
	``implementation'' get 	reduced to ``implement''. This helps reducing the amount of possible terms for searching and
	has been shown to increase performance \cite{saha2013}. The Porter stemmer is a popular choice for this process 
	\cite{porter1980}.
	
	\item An IR tool such as Lucene\footnote{https://lucene.apache.org/} is then used to store the processed representation
	of each source file to allow efficient search over the whole corpus.
\end{enumerate}

Once the index is formed the next step is building a query processor. In our case, the queries are bug reports posted 
on a bug-tracking system, and they normally consist of title and description. The same process outlined above, excluding 
the first three steps, is applied to both fields and an information retrieval system such as Lucene is used to rank the 
documents in order of descending relevance.

Relevance calculation is the step that will really determine the effectiveness of our implementation. Three different
values will be used for this process: textual similarity, file length and similar bugs. The way these values are computed
is presented next.

\subsection{Textual Similarity}
In the Vector Space Model, a source file is represented as a vector in which each dimension represents a term in
the corpus, and
its value is the amount of times that term appears in the document. For example, if we have a corpus consisting of the
terms $\{\text{black}, \text{funny}, \text{sheep}, \text{cat}\}$ and a document $d=\{\text{black}, \text{cat}, \text{funny},
\text{cat}\}$, the vector representation for this document will be $V_d = (1, 1, 0, 2)$. When comparing two vector
representations, in our case those referring to a bug report and a source file, simply calculating the distance
between the two points might not be the best approach \cite{manning2008}. Cosine similarity is
widely used in the IR field, and it consists on calculating the angle between the two vectors. This helps reduce the
bias towards longer documents, which is a big problem when using only euclidean distance. It is calculated as follows:

\begin{equation}
	S(q, d) = \cos(q, d) = \frac{\vec{V_q} \cdot \vec{V_d}}{|\vec{V_q}| |\vec{V_d}|}
	\label{similarity}
\end{equation}

In this equation, $q$ and $d$ are respectively a query and a document, and $\vec{V_q}$, $\vec{V_d}$ are their respective
vector representations.

A weighting scheme known as \textit{tf-idf} can be used to rank the results of a query. The rationale behind this method is
that not all terms in a corpus will have the same discriminating power. For instance, if a term is present in only one
document in the corpus, it could be assumed that it is a very specific term, and if it appears in a query, it is very
likely that the only document containing it will be relevant. On the other hand, if a term appears in every document in
the corpus, it will be difficult to rank the results based solely on it. Similarly, if a term appears many times in an
individual document, we can infer that the topic of that document is strongly related to that term.

Using this assumption we define two concepts: \textit{term frequency (tf)} and \textit{inverse document frequency (idf)}.
These values are calculated as follows:

\begin{equation}
	\text{tf}(t, d) = \log ({f_{\text{td}}})
\end{equation}

\begin{equation}
	\text{idf}(t, d) = \log \left( \frac{|D|}{n_t} \right) 
\end{equation}

Where $t$ represents a term, $d$ is a document, $f_{\text{td}}$ is the amount of times term $t$ appears in document $d$,
$|D|$ is the amount of documents in the corpus, and $n_t$ is the amount of documents in the corpus that contain term $t$.
The logarithms help dampen the effect of differences in document length even further. In this way we can define
\textit{tf-idf} as:

\begin{equation}
	\text{tf-idf}(t, d) = \text{tf}(t, d) \times \text{idf}(t, d)
\end{equation}

\subsection{File length}
As stated before, the size of the file does contribute to the probability of it containing the bug. In the case of having two files with the similar scores, the length of the file will be used in order to resolve conflicts.   
To include file length in the ranking calculation, the following equation is defined:

\begin{equation}
	g(d)= \frac{1}{1 + e ^ {-N(|d|)}}
\end{equation}

Where $|d|$ is the number of terms in the document. This function assigns higher scores to longer files. The value of $n$
is normalized using the following function:

\begin{equation}
	N(x) = \frac{x - x_\text{min}}{x_\text{max} - x_\text{min}}
\end{equation}

In which $x_\text{min}$ and $x_\text{max}$ are the lengths of the shortest and longest documents in the corpus,
respectively.

\subsection{Similar bugs}
Similar bugs will also be taken into account in order to improve the ranking system. This follows the idea that bugs that produce the same or similar erroneous behavior tend to be located in the same files. 
The results will also be augmented with information from previously fixed bug reports. For this, we consider a bug
$B$ as a query, then we collect a set $P$ of previously fixed bugs and a set $F$ of the files fixed to address
these bugs. The documents in $P$ are all of those that have a non-zero similarity value with the current bug being
considered, calculated using equation \ref{similarity}. For every document $f$ in $F$ we calculate:

\begin{equation}
	\text{SScore} = \sum_{P_i \in R(f)} \frac{S(B, P_i)}{n_i}
\end{equation}

Where $S$ is the similarity score defined in equation \ref{similarity}, $R(f)$ is the set of bug reports that include
$f$ in their fix set and $n_i$ is the amount of files in the fix set of $P_i$. This information is used to improve the location of bugs that have been previously identified in specif files.

\section{Experimental Design}
After the IR approach is implemented, it will be empirically tested on some open source software projects. 
A starting gold set has been acquired. It contains 424 bug reports in total, extracted manually from 9 open
source projects: BookKeeper, Derby, Lucene, Mahout, OpenJPA, Pig, Solr, Tika and Zookeeper, all of them developed
by The Apache Software Foundation\footnote{http://www.apache.org/}. Some of the projects have only a few data
points, so we intend to add more, attempting to have a comparable amount of bug reports for each system.

The implemented IR approach will be tested by performing search over the indexed source code using each query 
(bug report) differentiating between the results obtained using only the title of the bug report as a query, using 
only the description and using both. The results will be evaluated using the following IR metrics
\cite{saha2013, manning2008}:

\begin{itemize}
	\item Precision, which is the ratio of relevant documents retrieved out of the total amount of documents
	retrieved for a query.
	
	\item Recall, which refers to the ratio of relevant documents retrieved to the total number of relevant
	documents in the corpus for a query.
	
	\item Top $N$ Rank is the amount of bugs for which at least one of their relevant documents can be found
	within the top $N$ results where $N=1,5,10$. We will consider the bug located if at least one file from the
	relevant set of each query is present in the results.
	
	\item Reciprocal Rank is the multiplicative inverse of the rank of the first relevant document in a result
	set for a particular query. Evidently, it is desirable for this value to be close to $1$.
	
	\item Average Precision is used to get an overview of the performance of the system for a result list when
	the query has multiple relevant documents. It is calculated with the following function:
	\begin{equation}
		\text{AvgP} = \frac{\sum_{k=1}^{n} (P(k) \times pos(k))}{r}
	\end{equation}
	Where $n$ is the amount of documents retrieved, $P(k)$ is the precision at rank $k$, $r$ is the amount of relevant
	documents in the corpus and $pos(k)$ is $1$ if the document at position $k$ is relevant and $0$ otherwise.
\end{itemize}

\section{Conclusion}
With our implementation, we aim at developing a tool for bug localization at the file level that takes as input bug reports. This implementation follows a similar technique used in already known tools such as BugLocator. We will also test the effectiveness of our tool with 9 open source projects and 424 bugs reports already acquired and report future course of action. With these results, a comprehensive analysis of the strengths and weaknesses of 
the approach will be carried out, which will allow us to propose candidates for future work.

\bibliographystyle{abbrv}
\bibliography{project}
\balancecolumns

\end{document}
